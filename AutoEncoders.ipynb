{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoEncoders.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSup4v8p5nd3gUl2YO3Btm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUyHJTW17cxy"
      },
      "source": [
        "# Importing stuff\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dwSgz4zAsI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89f12eba-3252-47d3-f9ee-d9ef195e10b6"
      },
      "source": [
        "# Using mnist ;)\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshaping the data\n",
        "x_train = x_train.astype('float32')/255.\n",
        "x_test = x_test.astype('float32')/255.\n",
        "\n",
        "x_train=x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))\n",
        "x_test=x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiMtUNQwBE0_"
      },
      "source": [
        "# This is a shallow undercomplete autoencoder, which has \n",
        "input_l = layers.Input(shape=(784,)) # one input Layer\n",
        "bottleneck = layers.Dense(32, activation='relu')(input_l)# A bottleneck \n",
        "output_l = layers.Dense(784, activation='sigmoid')(bottleneck) # An output layer\n",
        "\n",
        "autoencoder = keras.Model(inputs = [input_l], outputs=[output_l]) # Creating the model\n",
        "\n",
        "encoder = keras.Model(inputs=[input_l], outputs=[bottleneck]) # The encoder\n",
        "\n",
        "encoded_input=layers.Input(shape=(32,)) # enocded data going to the bottleneck \n",
        "decoded = autoencoder.layers[-1](encoded_input) # data coming out of bottleneck layer\n",
        "\n",
        "decoder = keras.Model(inputs=[encoded_input], outputs=[decoded]) # Creating the decoder"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjxGJdngGFlA",
        "outputId": "d524737f-37a8-492a-9bdf-1ff0991a2060"
      },
      "source": [
        "# Compiling and training the model\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(\n",
        "    x_train, x_train,\n",
        "    epochs=50,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test, x_test)\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.2772 - val_loss: 0.1884\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1701 - val_loss: 0.1531\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1437 - val_loss: 0.1332\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1280 - val_loss: 0.1210\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1179 - val_loss: 0.1129\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1111 - val_loss: 0.1073\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1063 - val_loss: 0.1031\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1027 - val_loss: 0.1001\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.0999 - val_loss: 0.0975\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0978 - val_loss: 0.0957\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0965 - val_loss: 0.0947\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0956 - val_loss: 0.0940\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.0950 - val_loss: 0.0934\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0945 - val_loss: 0.0930\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0942 - val_loss: 0.0929\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0940 - val_loss: 0.0925\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0938 - val_loss: 0.0924\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0936 - val_loss: 0.0924\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0935 - val_loss: 0.0921\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0934 - val_loss: 0.0920\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0933 - val_loss: 0.0920\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0932 - val_loss: 0.0919\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0931 - val_loss: 0.0918\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0930 - val_loss: 0.0918\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0930 - val_loss: 0.0919\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0930 - val_loss: 0.0917\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0929 - val_loss: 0.0916\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0929 - val_loss: 0.0917\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0928 - val_loss: 0.0917\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0928 - val_loss: 0.0916\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0928 - val_loss: 0.0915\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0916\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0915\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0927 - val_loss: 0.0914\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0916\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0915\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0914\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.0926 - val_loss: 0.0914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb069f8a510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QywjNHeRHly"
      },
      "source": [
        "# Creating a Deep undercomplete autoencoder\n",
        "\n",
        "input_l=layers.Input(shape=(784,)) # input layers\n",
        "\n",
        "encoding_1=layers.Dense(256, activation='relu')(input_l) # 1st encoding layer\n",
        "encoding_2=layers.Dense(128, activation='relu')(encoding_1) # 2nd encoding layer\n",
        "\n",
        "bottleneck=layers.Dense(32, activation='relu')(encoding_2) # bottleneck layer\n",
        "\n",
        "decoding_1=layers.Dense(128, activation='relu')(bottleneck) # 1st decoding layer\n",
        "decoding_2=layers.Dense(256, activation='relu')(decoding_1) # 2nd decoding layer \n",
        "\n",
        "output_l=layers.Dense(784, activation='sigmoid')(decoding_2) # output layer\n",
        "\n",
        "autoencoder=keras.Model(inputs=[input_l],outputs=[output_l]) # creating the model\n",
        "\n",
        "encoder=keras.Model(inputs=[input_l],outputs=[bottleneck]) # creating the encoder and stuff\n",
        "encoded_input=layers.Input(shape=(32,))\n",
        "\n",
        "decoded_layer_1=autoencoder.layers[-3](encoded_input) # Creating the decoder layer and stuff\n",
        "decoded_layer_2=autoencoder.layers[-2](decoded_layer_1)\n",
        "\n",
        "decoded=autoencoder.layers[-1](decoded_layer_2)\n",
        "decoder=keras.Model(inputs=[encoded_input],outputs=[decoded]) # creating the model"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Bh4sqzgOYr",
        "outputId": "d1fe124e-3234-4714-a454-de3a6fadb2f7"
      },
      "source": [
        "# Creating a Variational autoencoder\n",
        "\n",
        "input_l = layers.Input(shape=(784,)) # input layer\n",
        "\n",
        "# The difference is that this time we will apply regularizers on our activation functions\n",
        "encoding_1=layers.Dense(256, activation='relu', activity_regularizer=keras.regularizers.l1(0.001))(input_l)\n",
        "bottleneck=layers.Dense(32, activation='relu', activity_regularizer=keras.regularizers.l1(0.001))(encoding_1)\n",
        "decoding_1=layers.Dense(256, activation='relu', activity_regularizer=keras.regularizers.l1(0.001))(bottleneck)\n",
        "output_l=layers.Dense(784, activation='sigmoid')(decoding_1)\n",
        "\n",
        "autoencoder = keras.Model(inputs=[input_l], outputs=[output_l]) # creating the model\n",
        "\n",
        "encoder=keras.Model(inputs=[input_l],outputs=[bottleneck])\n",
        "encoded_input=layers.Input(shape=(32,))\n",
        "decoded_layer_2=autoencoder.layers[-2](encoded_input)\n",
        "decoded=autoencoder.layers[-1](decoded_layer_2)\n",
        "decoder=keras.Model(inputs=[encoded_input],outputs=[decoded])\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "235/235 [==============================] - 6s 23ms/step - loss: 0.3740 - val_loss: 0.2823\n",
            "Epoch 2/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.2554 - val_loss: 0.2306\n",
            "Epoch 3/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.2181 - val_loss: 0.2063\n",
            "Epoch 4/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.2001 - val_loss: 0.1926\n",
            "Epoch 5/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1878 - val_loss: 0.1810\n",
            "Epoch 6/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1781 - val_loss: 0.1739\n",
            "Epoch 7/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1713 - val_loss: 0.1676\n",
            "Epoch 8/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1659 - val_loss: 0.1624\n",
            "Epoch 9/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1615 - val_loss: 0.1588\n",
            "Epoch 10/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1580 - val_loss: 0.1554\n",
            "Epoch 11/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1548 - val_loss: 0.1524\n",
            "Epoch 12/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1521 - val_loss: 0.1496\n",
            "Epoch 13/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1496 - val_loss: 0.1473\n",
            "Epoch 14/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1474 - val_loss: 0.1452\n",
            "Epoch 15/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1455 - val_loss: 0.1432\n",
            "Epoch 16/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1437 - val_loss: 0.1419\n",
            "Epoch 17/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1420 - val_loss: 0.1396\n",
            "Epoch 18/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1404 - val_loss: 0.1384\n",
            "Epoch 19/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1389 - val_loss: 0.1371\n",
            "Epoch 20/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1375 - val_loss: 0.1362\n",
            "Epoch 21/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1363 - val_loss: 0.1347\n",
            "Epoch 22/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1352 - val_loss: 0.1338\n",
            "Epoch 23/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1341 - val_loss: 0.1327\n",
            "Epoch 24/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1331 - val_loss: 0.1317\n",
            "Epoch 25/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1321 - val_loss: 0.1304\n",
            "Epoch 26/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1311 - val_loss: 0.1299\n",
            "Epoch 27/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1303 - val_loss: 0.1289\n",
            "Epoch 28/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1294 - val_loss: 0.1278\n",
            "Epoch 29/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1286 - val_loss: 0.1269\n",
            "Epoch 30/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1279 - val_loss: 0.1267\n",
            "Epoch 31/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1271 - val_loss: 0.1261\n",
            "Epoch 32/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1265 - val_loss: 0.1251\n",
            "Epoch 33/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1260 - val_loss: 0.1249\n",
            "Epoch 34/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1254 - val_loss: 0.1242\n",
            "Epoch 35/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1248 - val_loss: 0.1238\n",
            "Epoch 36/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1243 - val_loss: 0.1231\n",
            "Epoch 37/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1238 - val_loss: 0.1229\n",
            "Epoch 38/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1235 - val_loss: 0.1238\n",
            "Epoch 39/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1231 - val_loss: 0.1221\n",
            "Epoch 40/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1226 - val_loss: 0.1216\n",
            "Epoch 41/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1222 - val_loss: 0.1214\n",
            "Epoch 42/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1219 - val_loss: 0.1206\n",
            "Epoch 43/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1216 - val_loss: 0.1202\n",
            "Epoch 44/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1213 - val_loss: 0.1206\n",
            "Epoch 45/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1209 - val_loss: 0.1204\n",
            "Epoch 46/50\n",
            "235/235 [==============================] - 5s 22ms/step - loss: 0.1206 - val_loss: 0.1198\n",
            "Epoch 47/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1202 - val_loss: 0.1194\n",
            "Epoch 48/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1199 - val_loss: 0.1190\n",
            "Epoch 49/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1196 - val_loss: 0.1185\n",
            "Epoch 50/50\n",
            "235/235 [==============================] - 5s 23ms/step - loss: 0.1192 - val_loss: 0.1183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb05c760c90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}